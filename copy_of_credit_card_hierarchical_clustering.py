# -*- coding: utf-8 -*-
"""Copy of credit-card-Hierarchical Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mat1jyYhEpiOFC5AGIp-2GR1K9XwTdqN
"""

import numpy as np # linear algebra
import pandas as pd # data processing

df = pd.read_csv("CC GENERAL.csv")

"""##Extracting data and cleaning"""

df.info()

"""###There are a few null values, lets confirm that:

"""

df.isnull().sum()

"""##There are very few null values in credit limit and minimum payments column. Lets impute these values later."""

df.describe()

df.head(10)

"""##Cust_ID is one column which we will not be needing for model building, so lets drop it:"""

df = df.drop(['CUST_ID'],axis = 1)

df.head(10)

"""## Data Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns

"""##Univariate Analysis :

##Let's first identify the distribution of each column :
"""

for column in df.columns:
  plt.figure(figsize = (30,5))
  sns.histplot(df[column])
  plt.show()

"""###We observe the following trends here :

Most credit card holders have low credit limit and maintain credit balance below 7500

Variable such as Purchases, OneOffPurchases, installmentpurchases and cash advances also follow the same trend as credit balance. They could all be related. That is as the credit balance is low, the purchases are also low and so on

Most people either don't purchase anything or they purchase very frequently
People who purchase in installments is more than people who purchase in one-go

In the last 6 months, most people have made total payments below 10000, with the minimum payments below 5000

Finally, most of the credit card holders own a card for more than 12 months

###We can validate some of these trends further in bivariate analysis.
"""

for column in df.columns:
    plt.figure(figsize = (30,5))
    sns.boxplot(df[column])
    plt.show()

"""###We observe a lot of outliers here, hence deleting these outliers is not recommended. We could have deleted the outlier points if the number of outliers are less. Here, we will avoid deleting the outlier records. What we can do here based on column description is to normalize the data

###One more thing to observe here is minimum payments column also contains a lot of outliers, so lets impute the null values with median. You can also delete the records with null values and proceed since, the number of records with null values are small.

##Imputing null values :
"""

#imputing with median values using sklearn.impute
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='median')

X = df['MINIMUM_PAYMENTS'].values.reshape(-1,1)
X = imputer.fit_transform(X)

df['MINIMUM_PAYMENTS_NEW'] = X

X2 = df['CREDIT_LIMIT'].values.reshape(-1,1)
X2 = imputer.fit_transform(X2)

df['CREDIT_LIMIT_NEW'] = X2

df = df.drop(['CREDIT_LIMIT','MINIMUM_PAYMENTS'],axis = 1)
df.info()

df.isnull().sum()

"""##We have removed all the null values from the dataset.

##Bivariate Analysis
"""

sns.pairplot(df)
plt.show()

"""##We observe some of the following trends here:

As the credit limit increase, the balance also increases hence a linear relationship
As the number of purchases increases, the number of "cash in advance" transactions decreases
As the credit balance is low, the purchases, oneoffpurchases and installments purchases are less. Thus validating our assumption from univariate analysis
Purchases, oneoffpurchases and installment purchases are all related linearly
As the credit balance is low, the "cash in advance" transactions are less
"""

plt.figure(figsize=(20,20))
corr_df = df.corr()
sns.heatmap(corr_df,annot=True)
plt.show()

"""## Model Building :"""

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df,test_size=0.2,random_state=42)

"""##Normalizing the values:"""

from sklearn.preprocessing import MinMaxScaler
mm = MinMaxScaler()
train_df = mm.fit_transform(train_df)
test_df = mm.transform(test_df)

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer()
train_df = pt.fit_transform(train_df)
test_df = pt.transform(test_df)

"""## K-MEANS :"""

from sklearn.cluster import KMeans

"""##Finding optimum number of clusters for grouping:"""

interclusterdistance = []

for clusters in range(1,20):
    km = KMeans(n_clusters = clusters,init ='k-means++', max_iter=300,random_state=42)
    km.fit(train_df)
    interclusterdistance.append(km.inertia_)

#plotting the values
plt.figure(figsize=(30,10))
plt.plot(range(1, 20), interclusterdistance, marker='o', color='r')
plt.xlabel('Number of clusters')
plt.ylabel('Inter Cluster Distance')
plt.show()

"""###We observe that till k = 6 the inter cluster distance decreases significantly. Post that it decreases slightly. Lets form 6 clusters and display them:"""

km = KMeans(n_clusters = 6,init ='k-means++', max_iter=300,random_state=42)
km.fit(train_df)
y_pred = km.predict(train_df)

cluster_df = pd.DataFrame(train_df,columns = df.columns)
cluster_df['clusters'] = y_pred
cluster_df.head(10)

cluster_df['clusters'].value_counts()

"""##Here, we observe most of the customers fall in 3rd cluster, while 2nd cluster contains the least amount of customers.

visualise using two variables like BALANCE and PURCHASES:
"""

X = cluster_df[['BALANCE','PURCHASES']].to_numpy()

interclusterdistance = []

for clusters in range(1,20):
    km = KMeans(n_clusters = clusters,init ='k-means++', max_iter=300,random_state=42)
    km.fit(X)
    interclusterdistance.append(km.inertia_)

#plotting the values
plt.figure(figsize=(30,10))
plt.plot(range(1, 20), interclusterdistance, marker='o', color='g')
plt.xlabel('Number of clusters')
plt.ylabel('Inter Cluster Distance')
plt.show()

"""Considering only BALANCE variable, we can choose k = 4"""

km = KMeans(n_clusters = 4,init ='k-means++', max_iter=300,random_state=42)
km.fit(X)
y_balance_pred = km.predict(X)

plt.scatter(X[y_balance_pred==0, 0], X[y_balance_pred==0, 1], s=100, c='red', label ='Cluster 1')
plt.scatter(X[y_balance_pred==1, 0], X[y_balance_pred==1, 1], s=100, c='blue', label ='Cluster 2')
plt.scatter(X[y_balance_pred==2, 0], X[y_balance_pred==2, 1], s=100, c='green', label ='Cluster 3')
plt.scatter(X[y_balance_pred==3, 0], X[y_balance_pred==3, 1], s=100, c='yellow', label ='Cluster 4')

plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='cyan', label = 'Centroids')
plt.show()

"""##DBSCAN

##DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.
"""

from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=2,min_samples=6)
dbscan.fit(train_df)
y_dbscan_pred = dbscan.labels_
y_dbscan_pred

dbscan_df = pd.DataFrame(train_df,columns = df.columns)
dbscan_df['clusters'] = y_dbscan_pred
dbscan_df.head(10)

dbscan_df['clusters'].value_counts()

"""##Here, using eps (minimum distance between two points) as 2 and minimum samples in a cluster as 6, we get 6 clusters with majority of them being in cluster 0

visualise using two variables like BALANCE and PURCHASES:
"""

X = dbscan_df[['BALANCE','PURCHASES']].to_numpy()

dbscan = DBSCAN(eps=0.075,min_samples=2)
dbscan.fit(X)
y_dbscan_pred = dbscan.labels_
y_dbscan_pred

dbscan_df['clusters'] = y_dbscan_pred
dbscan_df['clusters'].value_counts()

plt.figure(figsize=(10,10))
plt.scatter(dbscan_df['BALANCE'],dbscan_df['PURCHASES'],c=dbscan_df['clusters'])
plt.title('DBSCAN Clustering',fontsize=15)
plt.xlabel('Feature 1',fontsize=10)
plt.ylabel('Feature 2',fontsize=10)
plt.show()

